{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2aDs6H/dmMwfKoxukuhZj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "S_RKSVoU-CBP"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import transformers\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get model, tokenizer, and max_length\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "max_length = tokenizer.model_max_length\n",
        "bos_token_id = tokenizer.bos_token_id\n"
      ],
      "metadata": {
        "id": "1ZKZyX0uHQ6g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download dataset from HF\n",
        "dataset = load_dataset(\n",
        "                path='EleutherAI/wikitext_document_level',\n",
        "                name='wikitext-2-raw-v1',\n",
        "                split='test'\n",
        "            )"
      ],
      "metadata": {
        "id": "SntrpOqq-RT4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = dataset[0]['page']\n",
        "print(sample_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMVzJvm4-xkj",
        "outputId": "3bead6da-60f5-413b-f18a-a0006e663c85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " = Robert Boulter = \n",
            " \n",
            " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \n",
            " In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . \n",
            " \n",
            " = = Career = = \n",
            " \n",
            " \n",
            " = = = 2000 – 2005 = = = \n",
            " \n",
            " In 2000 Boulter had a guest @-@ starring role on the television series The Bill ; he portrayed \" Scott Parry \" in the episode , \" In Safe Hands \" . Boulter starred as \" Scott \" in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . A review of Boulter 's performance in The Independent on Sunday described him as \" horribly menacing \" in the role , and he received critical reviews in The Herald , and Evening Standard . He appeared in the television series Judge John Deed in 2002 as \" Addem Armitage \" in the episode \" Political Expediency \" , and had a role as a different character \" Toby Steele \" on The Bill . \n",
            " He had a recurring role in 2003 on two episodes of The Bill , as character \" Connor Price \" . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . Boulter starred as \" Darren \" , in the 2005 theatre productions of the Philip Ridley play Mercury Fur . It was performed at the Drum Theatre in Plymouth , and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . Boulter received a favorable review in The Daily Telegraph : \" The acting is shatteringly intense , with wired performances from Ben Whishaw ( now unrecognisable from his performance as Trevor Nunn 's Hamlet ) , Robert Boulter , Shane Zaza and Fraser Ayres . \" The Guardian noted , \" Ben Whishaw and Robert Boulter offer tenderness amid the savagery . \" \n",
            " \n",
            " = = = 2006 – present = = = \n",
            " \n",
            " In 2006 Boulter starred in the play Citizenship written by Mark Ravenhill . The play was part of a series which featured different playwrights , titled Burn / Chatroom / Citizenship . In a 2006 interview , fellow actor Ben Whishaw identified Boulter as one of his favorite co @-@ stars : \" I loved working with a guy called Robert Boulter , who was in the triple bill of Burn , Chatroom and Citizenship at the National . He played my brother in Mercury Fur . \" He portrayed \" Jason Tyler \" on the 2006 episode of the television series , Doctors , titled \" Something I Ate \" . Boulter starred as \" William \" in the 2007 production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . In a review of the production for The Daily Telegraph , theatre critic Charles Spencer noted , \" Robert Boulter brings a touching vulnerability to the stage as William . \" \n",
            " Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris Leonti , and Donkey Punch directed by Olly Blackburn . Boulter portrayed a character named \" Sean \" in Donkey Punch , who tags along with character \" Josh \" as the \" quiet brother ... who hits it off with Tammi \" . Boulter guest starred on a two @-@ part episode arc \" Wounds \" in May 2008 of the television series Waking the Dead as character \" Jimmy Dearden \" . He appeared on the television series Survivors as \" Neil \" in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . He portrayed an emergency physician applying for a medical fellowship . He commented on the inherent difficulties in portraying a physician on television : \" Playing a doctor is a strange experience . Pretending you know what you 're talking about when you don 't is very bizarre but there are advisers on set who are fantastic at taking you through procedures and giving you the confidence to stand there and look like you know what you 're doing . \" Boulter starred in the 2011 film Mercenaries directed by Paris Leonti . \n",
            " \n",
            " = = Filmography = = \n",
            " \n",
            " \n",
            " = = = Film = = = \n",
            " \n",
            " \n",
            " = = = Television = = = \n",
            " \n",
            " \n",
            " = = = Theatre = = = \n",
            " \n",
            " \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Return a list of tuples that are (context, continuation)\n",
        "# This function specifies the context as 1 token at the start of each window\n",
        "# Note: It looks like for the last window lm eval harness assigns the \"extra space\" that is left over when the length of the last frame is not equal to the max_length\n",
        "# to more context tokens. In this excercise will follow the pattern of only having one context token per window\n",
        "def build_rolling_requests(encoded_tokens, max_length, bos_id):\n",
        "  requests = []\n",
        "  # Keep track of the tokens predicted so we know where to index and stop\n",
        "  tokens_predicted = 0\n",
        "  # Treat the first sequence different to specify the BOS token\n",
        "  first_sequence_length = min(max_length, len(encoded_tokens))\n",
        "  first_sequence = ([bos_id], encoded_tokens[:first_sequence_length])\n",
        "  requests.append(first_sequence)\n",
        "  tokens_predicted += first_sequence_length\n",
        "  # Loop while the predicted tokens is less than the length of encoded tokens\n",
        "  while tokens_predicted < len(encoded_tokens):\n",
        "    # Prediction should be the minimum of the max length and what we still have to predict\n",
        "    next_pred_len = min(max_length, len(encoded_tokens) - tokens_predicted)\n",
        "    next_pred = encoded_tokens[tokens_predicted: tokens_predicted + next_pred_len]\n",
        "    # Context will be 1 in this case so we can just take the the last predicted token from the previous window\n",
        "    context = [encoded_tokens[tokens_predicted-1]]\n",
        "    requests.append((context, next_pred))\n",
        "    tokens_predicted += next_pred_len\n",
        "  return requests\n",
        "\n"
      ],
      "metadata": {
        "id": "8n42ZGmm_T4B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "model.to(device)\n",
        "#Based on the requests calculate the loglikelihood for each page\n",
        "def get_log_likelihood(model, requests):\n",
        "  results = []\n",
        "  summed_results = 0\n",
        "  for request in tqdm(requests):\n",
        "    context, continuation = request\n",
        "    #Convert the input to a tensor to be passed to the model\n",
        "    inp = torch.tensor(\n",
        "        (context + continuation)[:-1],\n",
        "        dtype=torch.long,\n",
        "        device=device,\n",
        "    )\n",
        "    # Add a batch dimension to the input tensor\n",
        "    inp = inp.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      logits = model(inp).logits\n",
        "    # Normalize logits vocab dimension using log_softmax\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "    #Convert continuation tokens to a tensor\n",
        "    cont_toks = torch.tensor(continuation, dtype=torch.long, device=device).unsqueeze(0)\n",
        "    # Slice logits to get the logits for the continuation tokens\n",
        "    # logits has the shape [batch_size, sequence_length, vocab]\n",
        "    # Slice the sequence from the end of the sequence starting at the length of tokens in the continuation sequence\n",
        "    log_probs_for_cont = log_probs[:, -cont_toks.shape[1] :, :]\n",
        "\n",
        "    # Get the log probabilities of the actual continuation tokens\n",
        "    compare_log_probs = torch.gather(log_probs_for_cont, 2, cont_toks.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # Sum the log likelihood for the continuation tokens\n",
        "    # Sum instead of multiply because log(a*b) = log(a) + log(b)\n",
        "    log_likelihood = float(compare_log_probs.sum())\n",
        "    summed_results += log_likelihood\n",
        "    results.append(log_likelihood)\n",
        "  return (summed_results, results)"
      ],
      "metadata": {
        "id": "3QeUACe2Tn4_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_perplexity(a, b):\n",
        "    return math.exp(-(sum(a) / sum(b)))\n",
        "def get_bits_per_byte(a, b):\n",
        "    return -(sum(a) / sum(b)) / math.log(2)\n",
        "#Process_results copied from LM harness\n",
        "def process_page(doc):\n",
        "    page_words = len(re.split(r\"\\s+\", doc[\"page\"]))\n",
        "    page_bytes = len(doc[\"page\"].encode(\"utf-8\"))\n",
        "    return (page_words, page_bytes)\n"
      ],
      "metadata": {
        "id": "NXJpUMbsHuq2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate dataset with a limit\n",
        "LIMIT = 2\n",
        "limited_dataset = dataset.select(range(LIMIT))\n",
        "results = []\n",
        "\n",
        "for i in limited_dataset:\n",
        "  page = i['page']\n",
        "  requests = build_rolling_requests(tokenizer.encode(page), max_length, bos_token_id)\n",
        "  page_loglikelihood, _ = get_log_likelihood(model, requests)\n",
        "  page_words, page_bytes = process_page(i)\n",
        "  results.append((page_loglikelihood, page_words, page_bytes))\n",
        "\n",
        "all_log_likelihoods, all_words, all_bytes = zip(*results)\n",
        "word_perplexity = weighted_perplexity(all_log_likelihoods, all_words)\n",
        "byte_perplexity = weighted_perplexity(all_log_likelihoods, all_bytes)\n",
        "bits_per_byte = get_bits_per_byte(all_log_likelihoods, all_bytes)\n",
        "print(f\"\\n\\nWord Perplexity: {word_perplexity}\")\n",
        "print(f\"\\nByte Perplexity: {byte_perplexity}\")\n",
        "print(f\"\\nBits Per Byte: {bits_per_byte}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKhx7-iHKav7",
        "outputId": "575cca05-5638-42c7-b2ad-229389708efc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1299 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 2/2 [00:13<00:00,  6.54s/it]\n",
            "100%|██████████| 6/6 [00:40<00:00,  6.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Word Perplexity: 55.790871197851814\n",
            "\n",
            "Byte Perplexity: 2.17720336666234\n",
            "\n",
            "Bits Per Byte: 1.1224761720516812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}