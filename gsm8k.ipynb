{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOt1YSrHa4LfSdqQWZAvbg0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A simple implementation of gsm8k data set with options for cot and few shot examples"
      ],
      "metadata": {
        "id": "BuhAhZN4Ew5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jshMtmaVGYgQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GSM8KEval:\n",
        "  def __init__(self, model_name: str):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    self.model.to('cuda')\n",
        "\n",
        "    if self.tokenizer.pad_token is None:\n",
        "          self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "  def load_dataset(self, path: str):\n",
        "    dataset = load_dataset(path, 'main')\n",
        "    self.train_dataset = dataset['train']\n",
        "    self.test_dataset = dataset['test']\n",
        "\n",
        "  def get_few_shot_data(self, num_few_shot: int = 0):\n",
        "    few_shot_examples = ''\n",
        "    train_data = self.train_dataset.select(range(num_few_shot))\n",
        "    for example in train_data:\n",
        "      few_shot_examples += (f'Question: {example[\"question\"]}\\nAnswer: {example[\"answer\"]}\\n')\n",
        "    self.few_shot_examples = few_shot_examples\n",
        "\n",
        "  def build_request(self, question: str, use_cot: bool = False):\n",
        "    if use_cot:\n",
        "      return f'{self.few_shot_examples}Question: {question} Solve this step by step.\\nAnswer:'\n",
        "    else:\n",
        "      return f'{self.few_shot_examples}Question: {question}\\nAnswer:'\n",
        "\n",
        "  def generate_output(self, request: str):\n",
        "    input = self.tokenizer(request, return_tensors=\"pt\", padding=True)\n",
        "    # Move input tensor to the same device as the model\n",
        "    input = {k: v.to(self.model.device) for k, v in input.items()}\n",
        "\n",
        "    # Define stop tokens\n",
        "    stop_tokens = ['Question:', '</s>', '<|im_end|>']\n",
        "    stop_token_ids = [self.tokenizer.encode(token, add_special_tokens=False)[0] for token in stop_tokens if self.tokenizer.encode(token, add_special_tokens=False)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = self.model.generate(\n",
        "          **input,\n",
        "          max_new_tokens=150,\n",
        "          do_sample=False,\n",
        "          pad_token_id=self.tokenizer.pad_token_id,\n",
        "          eos_token_id=stop_token_ids,\n",
        "      )\n",
        "    # Decode output for just what is generated by the model\n",
        "    input_sequence_length = input['input_ids'].shape[1]\n",
        "    generated_sequence = outputs[0][input_sequence_length:]\n",
        "    generated_text = self.tokenizer.decode(\n",
        "            generated_sequence,\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "    return generated_text\n",
        "\n",
        "  def extract_answer(self, output: str):\n",
        "    answer = output.split('####')[-1].strip().replace(',', '')\n",
        "\n",
        "    numbers_in_answer = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', answer)\n",
        "    if numbers_in_answer:\n",
        "      return float(numbers_in_answer[-1].replace(',', ''))\n",
        "\n",
        "    numbers_in_output = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', output)\n",
        "    if numbers_in_output:\n",
        "      # Remove commas and convert to float\n",
        "      return float(numbers_in_output[-1].replace(',', ''))\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def isExactmatch(self, output: str, answer: str):\n",
        "    # Format of final answer is ####answer, so get that number from string\n",
        "    final_answer = float(answer.split('####')[-1].strip().replace(',', ''))\n",
        "    if final_answer == output:\n",
        "      return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "81obImpRzmmy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_eval = GSM8KEval('gpt2')"
      ],
      "metadata": {
        "id": "tA7dEyad2wGs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_eval.load_dataset('openai/gsm8k')"
      ],
      "metadata": {
        "id": "5pNjr9BG35KO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIMIT = 5\n",
        "correct_answers = 0\n",
        "for i in tqdm(range(LIMIT)):\n",
        "  sample_eval.get_few_shot_data(1)\n",
        "  request = sample_eval.build_request(sample_eval.test_dataset[i]['question'], True)\n",
        "  output = sample_eval.generate_output(request)\n",
        "\n",
        "  answer = sample_eval.extract_answer(output)\n",
        "  is_correct = sample_eval.isExactmatch(answer, sample_eval.test_dataset[i]['answer'])\n",
        "  if is_correct:\n",
        "    correct_answers += 1\n",
        "print(f'Accuracy: {correct_answers}/{LIMIT}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hypWWH5C4KW9",
        "outputId": "1b863af7-d5f7-4484-f0c5-72edd473da23"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:03<00:00,  1.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}